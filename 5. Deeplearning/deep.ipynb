{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 1\n",
    "\n",
    "#### (a) Explain how you can implement DL in a real-world application.\n",
    "Implementing Deep Learning (DL) in a real-world application involves several steps, including problem understanding, data collection and preprocessing, model selection and architecture design, training, evaluation, deployment, and monitoring. Here's a high-level overview of how you can implement DL in a real-world application:\n",
    "\n",
    "\n",
    "#### (b) What is the use of Activation function in Artificial Neural Networks? What would be the problem if we don't use it in ANN networks.\n",
    "Activation functions play a crucial role in artificial neural networks (ANNs) by introducing non-linearity into the network. The primary purpose of activation functions is to determine the output of a neuron, which is then passed on to the next layer of neurons in the network. Without activation functions, ANNs would essentially be reduced to linear transformations, and the network's capacity to learn and model complex, non-linear relationships in data would be severely limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 2\n",
    "\n",
    "### Train a Pure ANN with less than 10000 trainable parameters using the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mmanoj\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 12)                9420      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 9550 (37.30 KB)\n",
      "Trainable params: 9550 (37.30 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the simplified ANN model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(12, activation='relu', input_shape=(28 * 28,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\mmanoj\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mmanoj\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.7173 - accuracy: 0.7802 - val_loss: 0.2891 - val_accuracy: 0.9197\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4037 - accuracy: 0.8828 - val_loss: 0.2270 - val_accuracy: 0.9364\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8984 - val_loss: 0.1971 - val_accuracy: 0.9426\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3165 - accuracy: 0.9068 - val_loss: 0.1791 - val_accuracy: 0.9486\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2971 - accuracy: 0.9122 - val_loss: 0.1664 - val_accuracy: 0.9523\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2748 - accuracy: 0.9181 - val_loss: 0.1545 - val_accuracy: 0.9548\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2614 - accuracy: 0.9224 - val_loss: 0.1501 - val_accuracy: 0.9577\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2516 - accuracy: 0.9232 - val_loss: 0.1418 - val_accuracy: 0.9598\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2427 - accuracy: 0.9267 - val_loss: 0.1385 - val_accuracy: 0.9597\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2351 - accuracy: 0.9305 - val_loss: 0.1326 - val_accuracy: 0.9622\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9593\n",
      "Test accuracy: 0.9592999815940857\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Regression Task using ANN\n",
    "\n",
    "### Note: You are feel free to use any Regression ML dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Regression task using an Artificial Neural Network (ANN). We'll use the california Housing Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "boston  = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ANN model for regression\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Output layer with 1 neuron for regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2689 (10.50 KB)\n",
      "Trainable params: 2689 (10.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 1.1771 - val_loss: 0.5016\n",
      "Epoch 2/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.4220 - val_loss: 0.4345\n",
      "Epoch 3/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3947 - val_loss: 0.4209\n",
      "Epoch 4/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3645 - val_loss: 0.3925\n",
      "Epoch 5/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3491 - val_loss: 0.3988\n",
      "Epoch 6/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3400 - val_loss: 0.3571\n",
      "Epoch 7/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3468 - val_loss: 0.3854\n",
      "Epoch 8/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3290 - val_loss: 0.3424\n",
      "Epoch 9/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3131 - val_loss: 0.3413\n",
      "Epoch 10/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3096 - val_loss: 0.3313\n",
      "Epoch 11/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3097 - val_loss: 0.3310\n",
      "Epoch 12/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3361 - val_loss: 0.3282\n",
      "Epoch 13/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2996 - val_loss: 0.3249\n",
      "Epoch 14/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2973 - val_loss: 0.3238\n",
      "Epoch 15/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3071 - val_loss: 0.3340\n",
      "Epoch 16/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2917 - val_loss: 0.3288\n",
      "Epoch 17/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2901 - val_loss: 0.3236\n",
      "Epoch 18/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3167 - val_loss: 0.3628\n",
      "Epoch 19/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.3504 - val_loss: 0.3213\n",
      "Epoch 20/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2861 - val_loss: 0.3230\n",
      "Epoch 21/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2813 - val_loss: 0.3132\n",
      "Epoch 22/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2805 - val_loss: 0.3047\n",
      "Epoch 23/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2773 - val_loss: 0.3349\n",
      "Epoch 24/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2792 - val_loss: 0.3065\n",
      "Epoch 25/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2796 - val_loss: 0.3047\n",
      "Epoch 26/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2733 - val_loss: 0.3189\n",
      "Epoch 27/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2733 - val_loss: 0.3123\n",
      "Epoch 28/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2727 - val_loss: 0.3046\n",
      "Epoch 29/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2740 - val_loss: 0.3120\n",
      "Epoch 30/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2707 - val_loss: 0.3034\n",
      "Epoch 31/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2720 - val_loss: 0.3062\n",
      "Epoch 32/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2787 - val_loss: 0.3070\n",
      "Epoch 33/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2840 - val_loss: 0.3087\n",
      "Epoch 34/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2665 - val_loss: 0.3004\n",
      "Epoch 35/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2654 - val_loss: 0.2967\n",
      "Epoch 36/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2616 - val_loss: 0.2995\n",
      "Epoch 37/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2623 - val_loss: 0.3016\n",
      "Epoch 38/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2615 - val_loss: 0.2999\n",
      "Epoch 39/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2608 - val_loss: 0.3071\n",
      "Epoch 40/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2615 - val_loss: 0.3031\n",
      "Epoch 41/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.3150\n",
      "Epoch 42/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2593 - val_loss: 0.3032\n",
      "Epoch 43/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2568 - val_loss: 0.3024\n",
      "Epoch 44/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2584 - val_loss: 0.2949\n",
      "Epoch 45/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2622 - val_loss: 0.3000\n",
      "Epoch 46/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2575 - val_loss: 0.3029\n",
      "Epoch 47/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2577 - val_loss: 0.3021\n",
      "Epoch 48/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2551 - val_loss: 0.2974\n",
      "Epoch 49/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2533 - val_loss: 0.3068\n",
      "Epoch 50/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2545 - val_loss: 0.2978\n",
      "Epoch 51/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2534 - val_loss: 0.2883\n",
      "Epoch 52/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2504 - val_loss: 0.2943\n",
      "Epoch 53/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2508 - val_loss: 0.3042\n",
      "Epoch 54/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2514 - val_loss: 0.2865\n",
      "Epoch 55/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2493 - val_loss: 0.2970\n",
      "Epoch 56/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2508 - val_loss: 0.2945\n",
      "Epoch 57/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2481 - val_loss: 0.2933\n",
      "Epoch 58/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2475 - val_loss: 0.2871\n",
      "Epoch 59/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2473 - val_loss: 0.2874\n",
      "Epoch 60/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2462 - val_loss: 0.2937\n",
      "Epoch 61/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2498 - val_loss: 0.2908\n",
      "Epoch 62/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2445 - val_loss: 0.2915\n",
      "Epoch 63/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2436 - val_loss: 0.2896\n",
      "Epoch 64/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2439 - val_loss: 0.2863\n",
      "Epoch 65/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2442 - val_loss: 0.2982\n",
      "Epoch 66/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2433 - val_loss: 0.2926\n",
      "Epoch 67/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2421 - val_loss: 0.2949\n",
      "Epoch 68/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2414 - val_loss: 0.3004\n",
      "Epoch 69/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2471 - val_loss: 0.2845\n",
      "Epoch 70/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2411 - val_loss: 0.2914\n",
      "Epoch 71/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2416 - val_loss: 0.3032\n",
      "Epoch 72/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2409 - val_loss: 0.2822\n",
      "Epoch 73/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2380 - val_loss: 0.2902\n",
      "Epoch 74/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2387 - val_loss: 0.2890\n",
      "Epoch 75/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2423 - val_loss: 0.3052\n",
      "Epoch 76/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2400 - val_loss: 0.2966\n",
      "Epoch 77/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2388 - val_loss: 0.3092\n",
      "Epoch 78/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2403 - val_loss: 0.2829\n",
      "Epoch 79/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2359 - val_loss: 0.2885\n",
      "Epoch 80/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2386 - val_loss: 0.2990\n",
      "Epoch 81/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2352 - val_loss: 0.2988\n",
      "Epoch 82/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2367 - val_loss: 0.2828\n",
      "Epoch 83/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2361 - val_loss: 0.2981\n",
      "Epoch 84/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2349 - val_loss: 0.2820\n",
      "Epoch 85/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2346 - val_loss: 0.2959\n",
      "Epoch 86/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2344 - val_loss: 0.2843\n",
      "Epoch 87/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2347 - val_loss: 0.2893\n",
      "Epoch 88/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2349 - val_loss: 0.3130\n",
      "Epoch 89/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2467 - val_loss: 0.2954\n",
      "Epoch 90/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2409 - val_loss: 0.2877\n",
      "Epoch 91/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2329 - val_loss: 0.2828\n",
      "Epoch 92/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2303 - val_loss: 0.2880\n",
      "Epoch 93/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2316 - val_loss: 0.2873\n",
      "Epoch 94/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2325 - val_loss: 0.2888\n",
      "Epoch 95/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2291 - val_loss: 0.2920\n",
      "Epoch 96/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2298 - val_loss: 0.2830\n",
      "Epoch 97/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2283 - val_loss: 0.2843\n",
      "Epoch 98/100\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2306 - val_loss: 0.2829\n",
      "Epoch 99/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2293 - val_loss: 0.2865\n",
      "Epoch 100/100\n",
      "413/413 [==============================] - 1s 1ms/step - loss: 0.2276 - val_loss: 0.2937\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 1ms/step - loss: 0.2729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27294883131980896"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 999us/step - loss: 0.2729\n",
      "Test loss (Mean Squared Error): 0.27294883131980896\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test loss (Mean Squared Error):', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 950us/step\n",
      "Mean Squared Error (MSE): 0.2729488008908582\n",
      "Root Mean Squared Error (RMSE): 0.522445021883507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Mean Squared Error (MSE):', mse)\n",
    "print('Root Mean Squared Error (RMSE):', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
